{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct my start_urls for all the cases.\n",
    "# As there are no \"previous/next\" link betweem cases,\n",
    "# we cannot use a typical recursive algorithm.\n",
    "# Thus, we use the fact that the case names are structured:\n",
    "# year + number + year\n",
    "# (This is surely not the best way!)\n",
    "\n",
    "last_year_in_data = 2018 # to be updated\n",
    "\n",
    "years = list(range(2018, last_year_in_data+1))\n",
    "years = [str(y) for y in years] # convert to string to concatenate with case numbers\n",
    "\n",
    "case_num = list(range(1, 60))\n",
    "# convert to string to concatenate with years:\n",
    "case_num = ['-' + str(c).zfill(3) + 'N' for c in case_num] # zfill adds 0 to the left\n",
    "\n",
    "# concatenate all string and years together:\n",
    "cases = []\n",
    "for y in years:\n",
    "    for c in case_num:\n",
    "        str(cases.append(y + c))\n",
    "\n",
    "# complete URLs:\n",
    "cases = ['https://www.ekr.admin.ch/f524/' + case + '.html' for case in cases]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to check:\n",
    "\n",
    "* 2014-005N -> several cases in history: 2015-048N, (and thus appears here?)\n",
    "    * https://www.ekr.admin.ch/f524/2015-043N.html#2016-020N ==> 2 seperate case number in URL -> use html?p=1 ?\n",
    "    * https://www.ekr.admin.ch/prestations/f524/2014-005N.html?p=1 il y a bcp de texte...\n",
    "    \n",
    "\n",
    "* 2017-029N is missing\n",
    "    * because no history? \n",
    "\n",
    "\n",
    "* 2016-020N missing too\n",
    "\n",
    "* have duplicates: 2015-043N, 2015-047N, 2017-010N (4), 2018-009N, 2018-015N (3), \n",
    "\n",
    "* the accents are not read well by Excel. Excel issue?\n",
    "\n",
    "* le titre de https://www.ekr.admin.ch/prestations/f524/2018-005N.html ne contient que <<\n",
    "\n",
    "__Decisions:__  \n",
    "* scrap https://www.ekr.admin.ch/f524/2014-005N.html no need the number twice! just an anchor\n",
    "* do not care about jugement en second instance, just extract everything and then remove duplicates if needed. because both have the full text!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Done! It took 1.1657781600952148 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because need to restart kernel\n",
    "import scrapy # version 2.2.0\n",
    "import time\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Running...\")\n",
    "\n",
    "class CFRSpider(scrapy.Spider):\n",
    "    name = \"CFR\" # Naming the spider if you  running more than one spider of this class simultaneously.\n",
    "\n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "       'https://www.ekr.admin.ch/f524/2014-005N.html#2014-005N',\n",
    "       'https://www.ekr.admin.ch/f524/2015-048N.html#2016-020N',\n",
    "    ]\n",
    "#     start_urls = cases\n",
    "\n",
    "    # What to do with the URL. \n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Yield a dictionary with the values we want.\n",
    "        yield {\n",
    "            \n",
    "            'case': response.xpath('body/div/div[2]/div/div/div/div/div/p/text()').extract_first(),\n",
    "            'name': response.xpath('body/div/div[2]/div/div/div/div/div/h2/text()').extract_first(),\n",
    "            'location': response.xpath('body/div/div[2]/div/div/div/div/div/p[2]/text()').extract_first(),\n",
    "            \n",
    "            # procedure history | Historique de la procédure | Verfahrensgeschichte | Cronistoria della procedura\n",
    "            'year': response.xpath('body/div/div[2]/div/div/div/div/div/table[1]/tr/td/text()').extract()[0],\n",
    "            'link': response.xpath('body/div/div[2]/div/div/div/div/div/table[1]/tr/td[2]/span/a/@href').extract_first(),\n",
    "            'history': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[1]/tr/td[3])').extract_first(),\n",
    "            \n",
    "            # keywords | Mots-clés | Stichwörter | Parole chiave\n",
    "            'authors': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[1]/td[2])').extract_first(),\n",
    "            'victims': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[2]/td[2])').extract_first(),\n",
    "            'means': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[3]/td[2])').extract_first(),\n",
    "            'social_env': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[4]/td[2])').extract_first(),\n",
    "            'ideology': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[5]/td[2])').extract_first(),\n",
    "            \n",
    "            # not sure yet what is the best for the text...\n",
    "            # there is no new div, so it is not possible to distinguish it from the table above in the hierarchy\n",
    "            # usually there are 2 elements: \"Synthèse\" and \"Dècision\", but sometimes more.\n",
    "            # we will try to get them separately, both title h3 and text p,\n",
    "            # with the idea to do some Pandas magic after\n",
    "            # we think that there are no more than 5 titles, let's start with 6 to be sure\n",
    "            'title_1': response.xpath('body/div/div[2]/div/div/div/div/div/h3[1]/text()').extract_first(),\n",
    "            'title_2': response.xpath('body/div/div[2]/div/div/div/div/div/h3[2]/text()').extract_first(),\n",
    "            'title_3': response.xpath('body/div/div[2]/div/div/div/div/div/h3[3]/text()').extract_first(),\n",
    "            'title_4': response.xpath('body/div/div[2]/div/div/div/div/div/h3[4]/text()').extract_first(),\n",
    "            'title_5': response.xpath('body/div/div[2]/div/div/div/div/div/h3[5]/text()').extract_first(),\n",
    "            'title_6': response.xpath('body/div/div[2]/div/div/div/div/div/h3[6]/text()').extract_first(),\n",
    "            # start at p[3], because the 2 first p are case and location\n",
    "            'text_1': response.xpath('string(body/div/div[2]/div/div/div/div/div/p[3])').extract_first(),\n",
    "            'text_2': response.xpath('string(body/div/div[2]/div/div/div/div/div/p[4])').extract_first(),\n",
    "            'text_3': response.xpath('string(body/div/div[2]/div/div/div/div/div/p[5])').extract_first(),\n",
    "            'text_4': response.xpath('string(body/div/div[2]/div/div/div/div/div/p[6])').extract_first(),\n",
    "            'text_5': response.xpath('string(body/div/div[2]/div/div/div/div/div/p[7])').extract_first(),\n",
    "            'text_6': response.xpath('string(body/div/div[2]/div/div/div/div/div/p[8])').extract_first()\n",
    "        }\n",
    "            \n",
    "        page = response.url.split(\"/\")[-1]\n",
    "        filename = 'case_%s' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename)\n",
    "        \n",
    "# Instantiate our crawler.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'test.json',       # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(CFRSpider)\n",
    "process.start()\n",
    "print(\"Done! It took \" + str(time.time() - t0) + \" seconds.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Done! It took 2.0776572227478027 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because need to restart kernel\n",
    "import scrapy # version 2.2.0\n",
    "import time\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Running...\")\n",
    "\n",
    "class CFRSpider(scrapy.Spider):\n",
    "    name = \"CFR\" # Naming the spider if you  running more than one spider of this class simultaneously.\n",
    "\n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "       'https://www.ekr.admin.ch/f524/2014-005N.html#2014-005N',\n",
    "       'https://www.ekr.admin.ch/f524/2015-048N.html#2016-020N',\n",
    "    ]\n",
    "#     start_urls = cases\n",
    "\n",
    "    # What to do with the URL. \n",
    "    def parse(self, response):\n",
    "        # store all the titles of the texts:\n",
    "        titles = []\n",
    "        for title in response.xpath('body/div/div[2]/div/div/div/div/div/h3/text()').getall():\n",
    "            titles.append(title)\n",
    "        \n",
    "        texts = []\n",
    "        for text in response.xpath('body/div/div[2]/div/div/div/div/div/p/text()').getall():\n",
    "            texts.append(text)\n",
    "        \n",
    "        # Yield a dictionary with the values we want.\n",
    "        yield {\n",
    "            \n",
    "            'case': response.xpath('body/div/div[2]/div/div/div/div/div/p/text()').extract_first(),\n",
    "            'name': response.xpath('body/div/div[2]/div/div/div/div/div/h2/text()').extract_first(),\n",
    "            'location': response.xpath('body/div/div[2]/div/div/div/div/div/p[2]/text()').extract_first(),\n",
    "            \n",
    "            # procedure history | Historique de la procédure | Verfahrensgeschichte | Cronistoria della procedura\n",
    "            'year': response.xpath('body/div/div[2]/div/div/div/div/div/table[1]/tr/td/text()').extract()[0],\n",
    "            'link': response.xpath('body/div/div[2]/div/div/div/div/div/table[1]/tr/td[2]/span/a/@href').extract_first(),\n",
    "            'history': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[1]/tr/td[3])').extract_first(),\n",
    "            \n",
    "            # keywords | Mots-clés | Stichwörter | Parole chiave\n",
    "            'authors': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[1]/td[2])').extract_first(),\n",
    "            'victims': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[2]/td[2])').extract_first(),\n",
    "            'means': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[3]/td[2])').extract_first(),\n",
    "            'social_env': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[4]/td[2])').extract_first(),\n",
    "            'ideology': response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[5]/td[2])').extract_first(),\n",
    "            \n",
    "            'titles': titles,\n",
    "            \n",
    "            'texts': texts[3:] # start at p[3], because the 2 first p are case and location\n",
    "            \n",
    "\n",
    "        }\n",
    "            \n",
    "        \n",
    "# Instantiate our crawler.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'test.json',       # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(CFRSpider)\n",
    "process.start()\n",
    "print(\"Done! It took \" + str(time.time() - t0) + \" seconds.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Done! It took 1.3137657642364502 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because need to restart kernel\n",
    "import scrapy # version 2.2.0\n",
    "import time\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Running...\")\n",
    "\n",
    "class CFRSpider(scrapy.Spider):\n",
    "    name = \"CFR\" # Naming the spider if you  running more than one spider of this class simultaneously.\n",
    "    allowed_domains = ['https://www.ekr.admin.ch/f524/']\n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "       'https://www.ekr.admin.ch/f524/2014-005N.html',\n",
    "       'https://www.ekr.admin.ch/f524/2016-020N.html',\n",
    "    ]\n",
    "#     start_urls = cases\n",
    "\n",
    "    # What to do with the URL. \n",
    "    def parse(self, response):\n",
    "        # the css style are not really useful in these webpages... thus we use xpath\n",
    "        \n",
    "        # store all the titles of the texts:\n",
    "        titles = []\n",
    "        for title in response.xpath('body/div/div[2]/div/div/div/div/div/h3/text()').getall():\n",
    "            titles.append(title)\n",
    "        \n",
    "        texts = []\n",
    "        for text in response.xpath('body/div/div[2]/div/div/div/div/div/p/text()').getall():\n",
    "            texts.append(text)\n",
    "         \n",
    "        # Yield a dictionary with the values we want.\n",
    "        item = {}\n",
    "        \n",
    "        item['case'] = response.xpath('body/div/div[2]/div/div/div/div/div/p/text()').extract_first()\n",
    "        item['name'] = response.xpath('body/div/div[2]/div/div/div/div/div/h2/text()').extract_first()\n",
    "        item['location'] = response.xpath('body/div/div[2]/div/div/div/div/div/p[2]/text()').extract_first()\n",
    "            \n",
    "        # procedure history | Historique de la procédure | Verfahrensgeschichte | Cronistoria della procedura\n",
    "        item['year'] = response.xpath('body/div/div[2]/div/div/div/div/div/table[1]/tr/td/text()').extract()[0]\n",
    "        item['link'] = response.xpath('body/div/div[2]/div/div/div/div/div/table[1]/tr/td[2]/span/a/@href').extract_first()\n",
    "        item['history'] = response.xpath('string(body/div/div[2]/div/div/div/div/div/table[1]/tr/td[3])').extract_first()\n",
    "            \n",
    "        # keywords | Mots-clés | Stichwörter | Parole chiave\n",
    "        item['authors'] = response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[1]/td[2])').extract_first()\n",
    "        item['victims'] = response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[2]/td[2])').extract_first()\n",
    "        item['means'] = response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[3]/td[2])').extract_first()\n",
    "        item['social_env'] = response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[4]/td[2])').extract_first()\n",
    "        item['ideology'] = response.xpath('string(body/div/div[2]/div/div/div/div/div/table[3]/tr[5]/td[2])').extract_first()\n",
    "            \n",
    "        item['titles'] = titles\n",
    "            \n",
    "        item['texts'] = texts[3:] # start at p[3], because the 2 first p are case and location\n",
    "                         \n",
    "        yield item\n",
    "\n",
    "            \n",
    "        \n",
    "# Instantiate our crawler.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'test.json',       # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(CFRSpider)\n",
    "process.start()\n",
    "print(\"Done! It took \" + str(time.time() - t0) + \" seconds.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Done! It took 9.980156183242798 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because need to restart kernel\n",
    "import scrapy # version 2.2.0\n",
    "import time\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Running...\")\n",
    "\n",
    "class CFRSpider(scrapy.Spider):\n",
    "    name = \"CFR\" # Naming the spider if you  running more than one spider of this class simultaneously.\n",
    "    allowed_domains = ['https://www.ekr.admin.ch/f524/']\n",
    "#     start_urls = [\n",
    "#         'https://www.ekr.admin.ch/f524/2014-005N.html',\n",
    "#         'https://www.ekr.admin.ch/f524/2016-020N.html'\n",
    "#     ]\n",
    "    start_urls = cases\n",
    "    \n",
    "    def parse(self, response):\n",
    "        item = {} # a dictionary to store the results\n",
    "        \n",
    "        item['case'] = response.xpath('body/div/div[2]/div/div/div/div/div/p/text()').extract_first()\n",
    "        item['name'] = response.xpath('body/div/div[2]/div/div/div/div/div/h2/text()').extract_first()\n",
    "        item['location'] = response.xpath('body/div/div[2]/div/div/div/div/div/p[2]/text()').extract_first()\n",
    "\n",
    "        # procedure history | Historique de la procédure | Verfahrensgeschichte | Cronistoria della procedura\n",
    "\n",
    "\n",
    "        text = [] # collect all the text with html tags in one list\n",
    "        # we do this, because there is no regular structure in the texts displayed\n",
    "        \n",
    "        # h3 titles that are not part of the interesting data:\n",
    "        titles_not_wanted = ['<h3>Actualité</h3>', '<h3>Thèmes</h3>', '<h3>Bases juridiques</h3>', '<h3>International</h3>', '<h3>Prestations</h3>', '<h3>Publications</h3>', '<h3>La CFR</h3>', '<h3>Restez informé</h3>']\n",
    "        \n",
    "        for cnt, h3_selector in enumerate(response.css('h3'), start=1):\n",
    "            key = h3_selector.extract() # <-with html tags | without-> xpath('normalize-space()').get() # get the h3 title's text\n",
    "            if key not in titles_not_wanted:\n",
    "                text.append(key) # append the h3 title\n",
    "                values = h3_selector.xpath('following-sibling::p[count(preceding-sibling::h3)=$cnt]', cnt=cnt).getall()\n",
    "                values = ''.join([str(elem) for elem in values])  # convert list to string (remove [])\n",
    "                text.append(values) # add the paragraphs <p> in between the <h3>\n",
    "        \n",
    "        item['html_text'] = ''.join(text) # all the text together, in a single string (with html tags)\n",
    "        item['html_text_as_list'] = text # all the text together, as a list (with html tags)\n",
    "\n",
    "        yield(item)\n",
    "    \n",
    "    \n",
    "# Instantiate our crawler.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'test.json',       # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(CFRSpider)\n",
    "process.start()\n",
    "print(\"Done! It took \" + str(time.time() - t0) + \" seconds.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>case</th>\n",
       "      <td>Cas 2018-002N</td>\n",
       "      <td>Cas 2018-001N</td>\n",
       "      <td>Cas 2018-006N</td>\n",
       "      <td>Cas 2018-008N</td>\n",
       "      <td>Cas 2018-010N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>Internationale Konferenz der Anti-Zensur-Koali...</td>\n",
       "      <td>Gens du voyage, travaux de construction</td>\n",
       "      <td>Hausfriedensbruchs in der Asylunterkunft</td>\n",
       "      <td>Provokativen Postkarten von Sharia_pride II</td>\n",
       "      <td>Rassistisches Video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>Grisons</td>\n",
       "      <td>Fribourg</td>\n",
       "      <td>Bâle-Campagne</td>\n",
       "      <td>Schwyz</td>\n",
       "      <td>Berne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>html_text_as_list</th>\n",
       "      <td>[&lt;h3&gt;Synthèse&lt;/h3&gt;, &lt;p&gt;Der Beschuldigte gründe...</td>\n",
       "      <td>[&lt;h3&gt;Synthèse&lt;/h3&gt;, &lt;p&gt;Plusieurs associations ...</td>\n",
       "      <td>[&lt;h3&gt;Synthèse&lt;/h3&gt;, &lt;p&gt;Der Beschuldigte betrat...</td>\n",
       "      <td>[&lt;h3&gt;Synthèse&lt;/h3&gt;, &lt;p&gt;In der Nacht wurden Pos...</td>\n",
       "      <td>[&lt;h3&gt;Synthèse&lt;/h3&gt;, &lt;p&gt;Der Beschuldigte – welc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>html_text_in_one</th>\n",
       "      <td>&lt;h3&gt;Synthèse&lt;/h3&gt;&lt;p&gt;Der Beschuldigte gründete ...</td>\n",
       "      <td>&lt;h3&gt;Synthèse&lt;/h3&gt;&lt;p&gt;Plusieurs associations fri...</td>\n",
       "      <td>&lt;h3&gt;Synthèse&lt;/h3&gt;&lt;p&gt;Der Beschuldigte betrat oh...</td>\n",
       "      <td>&lt;h3&gt;Synthèse&lt;/h3&gt;&lt;p&gt;In der Nacht wurden Postka...</td>\n",
       "      <td>&lt;h3&gt;Synthèse&lt;/h3&gt;&lt;p&gt;Der Beschuldigte – welcher...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   0  \\\n",
       "case                                                   Cas 2018-002N   \n",
       "name               Internationale Konferenz der Anti-Zensur-Koali...   \n",
       "location                                                     Grisons   \n",
       "html_text_as_list  [<h3>Synthèse</h3>, <p>Der Beschuldigte gründe...   \n",
       "html_text_in_one   <h3>Synthèse</h3><p>Der Beschuldigte gründete ...   \n",
       "\n",
       "                                                                   1  \\\n",
       "case                                                   Cas 2018-001N   \n",
       "name                         Gens du voyage, travaux de construction   \n",
       "location                                                    Fribourg   \n",
       "html_text_as_list  [<h3>Synthèse</h3>, <p>Plusieurs associations ...   \n",
       "html_text_in_one   <h3>Synthèse</h3><p>Plusieurs associations fri...   \n",
       "\n",
       "                                                                   2  \\\n",
       "case                                                   Cas 2018-006N   \n",
       "name                        Hausfriedensbruchs in der Asylunterkunft   \n",
       "location                                               Bâle-Campagne   \n",
       "html_text_as_list  [<h3>Synthèse</h3>, <p>Der Beschuldigte betrat...   \n",
       "html_text_in_one   <h3>Synthèse</h3><p>Der Beschuldigte betrat oh...   \n",
       "\n",
       "                                                                   3  \\\n",
       "case                                                   Cas 2018-008N   \n",
       "name                     Provokativen Postkarten von Sharia_pride II   \n",
       "location                                                      Schwyz   \n",
       "html_text_as_list  [<h3>Synthèse</h3>, <p>In der Nacht wurden Pos...   \n",
       "html_text_in_one   <h3>Synthèse</h3><p>In der Nacht wurden Postka...   \n",
       "\n",
       "                                                                   4  \n",
       "case                                                   Cas 2018-010N  \n",
       "name                                             Rassistisches Video  \n",
       "location                                                       Berne  \n",
       "html_text_as_list  [<h3>Synthèse</h3>, <p>Der Beschuldigte – welc...  \n",
       "html_text_in_one   <h3>Synthèse</h3><p>Der Beschuldigte – welcher...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_json('test.json', orient='records')\n",
    "print(test.shape)\n",
    "test.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h3>Synthèse</h3><p>Der Beschuldigte verfasste mit seinem Mobiltelefon auf dem Facebook-Profil der Zeitschrift Blick unter dem Artikel « <i>Zu wenig Tage in der Schweiz verbracht - Ägypter wird ausgeschafft </i>», die folgenden Kommentare: « <i>Na endlich! Christen zu Christen, Araber zu Araber! </i>»; «<i>Und genau deswegen MUSS der Islam bekämpft und ausgerottet werden </i>»; «<i>Und genau darum muss der Islam ausgerottet werden, Ich figge Allah i Arsch. </i>».<br>\\nNach Ansicht der Staatsanwaltschaft rief der Beschuldigte mit diesen Kommentaren auf einer öffentlich zugänglichen Internetplattform wissentlich und willentlich gegen Angehörige der Religion des Islams zum Hass auf und verstiess somit gegen Art. 261<sup>bis</sup> StGB.<br>\\n</p><h3>Décision</h3><p>Der Beschuldigte wird wegen Rassendiskriminierung schuldig erklärt und wird mit einer Geldstrafe von 60 Tagessätzen zu je CHF 30.00 und eine Busse von CHF 400.00 bestraft. Die Geldstrafe wird bedingt ausgesprochen bei einer Probezeit von 3 Jahren. Die Kosten des Verfahrens im Umfang von CHF 580.00 werden dem Beschuldigten auferlegt.<br>\\n</p>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test[\"En droit / considérants_4\"][0]\n",
    "test.head()\n",
    "# test[pd.notna(test[\"Décision_5\"])]\n",
    "''.join(test.html_text_as_list[12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<h3>Synthèse</h3>',\n",
       " '<p>Der Beschuldigte hat auf einem Fussballplatz in Anwesenheit von Kindern und anderen Zuschauern einen Mann als « <i>Dreckneger</i> » betitelt und ihm gesagt « <i>Wenn ich eine Banane werfen würde, würdest du wie ein Affe die Banane holen</i> ».Diese öffentliche Herabsetzung in der Menschenwürde verstösst nach Meinung der zuständigen Strafverfolgungsbehörde gegen das Verbot der Rassendiskriminierung in Art. 261<sup>bis</sup> Abs. 4 StGB, weshalb der Beschuldigte zu einer Geldstrafe verurteilt wurde.<br>\\n</p>',\n",
       " '<h3>Décision</h3>',\n",
       " '<p>Der Beschuldigte wird wegen Rassendiskriminierung (Art. 261<sup>bis</sup> Abs. 4 StGB), für schuldig erklärt und wird mit einer Geldstrafe von 30 Tagessätzen zu je CHF 50.00, ausmachend CHF 300.00 bestraft. Die Geldstrafe wird bedingt ausgesprochen bei einer Probezeit von 3 Jahren. Die Kosten des Verfahrens im Umfang von CHF 530.00 werden dem Beschuldigten auferlegt.<br>\\n</p>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.html_text_as_list[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data table as .csv\n",
    "\n",
    "test.to_csv(path_or_buf='test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
